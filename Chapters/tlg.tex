\chapter{Grammar}
\label{chapter:tlg}

\section{Background}
The theoretical framework on top of which this thesis is built are type-logical grammars, a particular family of categorial grammars.
This chapter aims to provide a brief introductory background on type-logical grammars and their historical origins, positioning them within the broader context of categorial grammars and exposing their distinguishing characteristics.
Afterwards, an account of the specific grammar instantiation used for the current work will be given, in terms of its logical and and computational bases, together with the motivating reasons for its choice.
Key references for this chapter are the Stanford Encyclopedia of Philosophy entry for Type-Logical Grammars~\cite{sep-typelogical-grammar} and Moot and Retor{\'e}'s book on Categorial Type Logics~\cite{moot2012logic}.

\subsection{Overview}
\paragraph{Categorial Grammars}
Categorial grammar formalisms have their origins in the works of  Adjukiewicz~\cite{ajdukiewicz1935syntaktische} and Bar-Hillel~\cite{bar1953quasi}.
At their core and ever since their inception, they are defined on the basis of two simple components; a type system and a set of rules dictating type interactions.
The former is an inductive scheme for category (or type) construction, that utilizes a set of atomic types and a set of type-forming operators to provide the means for creating complex types.
The latter provides a number of schemata that describe what kinds of type combinations are permitted, and what the productions of these combinations are.

Categorial Grammars treat syntax as the formal process that dictates how phrases are gradually built by their components, which combine with one-another in terms of function-argument relations. 
They thus epitomize on the \textit{principle of compositionality}, which posits that the meaning of complex expressions is a production of the meaning of their parts and the rules used to compose them.

\paragraph{Parsing as Deduction}
The key insight of Type-Logical Grammars and their distinguishing feature is the logical take on the parsing process.
Lambek was the first to notice that categories may be perceived as logical formulas, and type-forming operators as logical connectives~\cite{lambek1958mathematics}.
Parsing is then lifted from arbitrary schematic rule application to a process of deductive inference, as driven by an underlying logic.
This yields a number of benefits which will be clarified later; for now, it is worth noting the flexibility inherent to such an approach.
Altering the choice of logic gives rise to a different grammar, so a new grammar may be designed for particular use-cases by a adopting an appropriate logic.
As such, Type-Logical Grammars form a wide landscape which encompasses many formalisms which may differ in their properties but all share a proof-theoretic perspective on parsing.

\subsection{Type-Logical Grammars}
Although a full exposition and comparison between the various incarnations of Type-Logical Grammars escapes the purposes of this work, it is still worthwhile to inspect their persistent aspects and their historical origins.

\paragraph{Lambek Calculus}
We begin with a brief description of what has come to be known as the Lambek Calculus (L)~\cite{lambek1958mathematics}, which built upon AB Grammars~\cite{bar1953quasi} in providing them with a logical formalization.
Categories are defined as follows:
\[
\textsc{c} := \textsc{a} \ | \ \textsc{c}_1/\textsc{c}_2 \ | \ \textsc{c}_1 \backslash \textsc{c}_2
\]
This inductive scheme states that a valid category is either an atomic category $\textsc{a} \in \mathcal{A}$, where $\mathcal{A}$ a closed set of categories, or the result of either of the binary operators $/$, $\backslash$ (read as slash and backslash) on two valid categories.
Intuitively, an atomic element $\textsc{a}$ corresponds to a complete category, whereas a complex category $\textsc{a}/\textsc{b}$ ($\textsc{a} \backslash \textsc{b}$) correspond to an incomplete (or fractional) category that misses a $\textsc{b}$ to the right (left) to produce a full category $\textsc{a}$, with $/$ and $\backslash$ acting as directional \textit{implications}.
The corresponding type-logic contains four logical rules, presented here in Natural Deduction style:
\begin{align*}
    \begin{minipage}{0.5\textwidth}
    \begin{align*}
        \infer{\Gamma, \Delta \vdash B}{
            \Gamma \vdash B / A
            &
            \Delta \vdash A
        }\tag{/E}\\
        \\
        \infer{\Gamma \vdash B/A}{
            \Gamma, A \vdash B
        }\tag{/I}
    \end{align*}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \begin{align*}
        \infer{\Delta, \Gamma \vdash B}{
            \Delta \vdash A
            &
            \Gamma \vdash A\backslash B
        }\tag{$\backslash E$}\\
        \\
        \infer{\Gamma \vdash A\backslash B}{
            A, \Gamma \vdash B
        }\tag{$\backslash I$}
    \end{align*}
    \end{minipage}
\end{align*}
plus the identity Axiom:
\[
\infer{A \vdash A}{}\tag{Ax.}
\]
where $A$, $B$ are formulas (i.e. categories) and $\Gamma$, $\Delta$ are sequences of formulas. 
A statement of the form $\Gamma \vdash A$ is a judgement, expressing that from a sequence of \textit{assumptions} $\Gamma$ one can derive a \textit{conclusion} formula $A$.

The first line presents the slash and backslash Elimination rules, where $/E$ ($\backslash E$) states that if one has a proof of a formula $B/A$ ($A \backslash B$) from assumptions $\Gamma$ and a proof of a formula $A$ from assumptions $\Delta$, then from assumptions $\Gamma$, $\Delta$ ($\Delta$, $\Gamma$) one can derive a formula $B$.
Note that $\Gamma$, $\Delta$ refers to the concatenation of $\Gamma$ to $\Delta$ and is distinct from $\Delta$, $\Gamma$  --- that is, the order of items within a sequence plays a role in what constitutes a valid proof since our logical rules are non-commutative.
The second line presents the corresponding Introduction rules.
Now, $/I$ ($\backslash I$) state that if ones know a sequence $\Gamma$, $A$ to derive a $B$, then $A$ may be withdrawn, allowing one to derive $B/A$ ($B \backslash A$) from $\Gamma$ alone.
Elimination rules are dual to Introduction rules; the first allow the removal of an implicational type by applying it to its argument, whereas the latter create implicational types by abstracting arguments away, giving our logic access to \textit{hypothetical reasoning}.

% syntax ~hom~> semantics
% non-associative fragment lambek 61
% structural control modalities
% practical concesion => BALANCE stricter proof-theoretic properties, larger type-ambiguity (how do we come up with the proper type assignment?)
% for languages with free word order the type assignments become even harder (either due to modalities or distinct types)
% => next section: a lax proof-theoretic version that is easier to type-assign
% dependency informaiton to recover from loss of directionality
% dependency information ARE modal operators
% examples
% potential semantic interpretations..

To illustrate the linguistic relevance of such a grammar, we will take a second  to review how implication types may be used to convey information on sentence structure.
First off, atomic categories may be seen as structurally complete, independent phrases.
Phrasal composition is coordinated by phrasal heads, which are assigned complex categories.
Heads are then functors which consume the categories of their dependants, producing as a result the wider phrasal category.
At the bottom level, categories are provided by a \textit{lexicon}, a binary relation which associates lexical entries (i.e. words) with one or more potential categories.

To get the point across, we can devise a minimal grammar capable of modeling the syntactic structure of a small set of example sentences.
 Let us first initialize an atomic category set consisting of the elements $\textsc{n}$ for noun, $\textsc{np}$ for noun-phrase and $\textsc{s}$ for sentence, and a corresponding lexicon as follows:
 \[
 \begin{array}{cc}
 \text{Word} & \text{Category} \\
 \hline
 \text{girl}, \ \text{apple} & \textsc{n} \\
 \text{children} & \textsc{np} \\
 \text{the}, \ \text{a(n)} & \textsc{np}/ \textsc{n} \\
 \text{play(s)} & \textsc{np}\backslash \textsc{s} \\
 \text{ate} & (\textsc{np} \backslash \textsc{s}) / \textsc{np} \\ 
 \text{who} & (\textsc{np} \backslash \textsc{np}) / (\textsc{np} \backslash \textsc{s}) \\
 \text{which} & (\textsc{np} \backslash \textsc{np})/(\textsc{s} / \textsc{np})
 \end{array}
 \]

Equipped with the above lexicon, we can use the Lambek Calculus to provide derivations for a number of simple examples as shown in Figure~\ref{fig:lambek_en}, involving usage of our versions of intransitive ``play'' and transitive ``ate'' both in primary and embedded clauses.
Notice how the introduction rules bypass the need for explicit combinatory rules for peripheral extraction, as seen in the object-relativisation example of~\ref{fig:lambek_en:obj}.

 \begin{figure}
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \[
         \infer[\backslash E]{\text{children}, \text{play} \vdash \textsc{s}}{
             \infer[L]{\text{children} \vdash \textsc{np}}{}
             &
             \infer[L]{\text{play} \vdash \textsc{np}\backslash \textsc{s}}{}
         } 
         \]
         \caption{Simple intransitive verb derivation.}
     \end{subfigure}
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \[
         \infer[\backslash E]{\text{the}, \text{girl}, \text{ate}, \text{an}, \text{apple} \vdash \textsc{s}}{
             \infer[/E]{\text{the}, \text{girl} \vdash \textsc{np}}{
                 \infer[L]{\text{the} \vdash \textsc{np} / \textsc{n}}{}
                 &
                 \infer[L]{\text{girl} \vdash \textsc{n}}{}
             }
             &
             \infer[/E]{\text{ate}, \text{an}, \text{apple} \vdash \textsc{np} \backslash \textsc{s}}{
                 \infer[L]{\text{ate} \vdash (\textsc{np} \backslash \textsc{s})/\textsc{np}}{}
                 &
                 \infer[/E]{\text{an}, \text{apple} \vdash \textsc{np}}{
                     \infer[L]{\text{an} \vdash \textsc{np}/ \textsc{n}}{}
                     &
                     \infer[L]{\text{apple} \vdash \textsc{n}}{}
                 }
             }
         }
         \]
         \caption{Simple transitive verb phrase derivation.}
     \end{subfigure}
     \begin{subfigure}[b]{1\textwidth}
     \[
     \infer[/ E]{\text{who}, \text{play} \vdash \textsc{np} \backslash \textsc{np}}{
     	\infer[L]{\text{who} \vdash (\textsc{np}\backslash \textsc{np}) / (\textsc{np} \backslash \textsc{s})}{}
     	&
     	\infer[L]{\text{play} \vdash \textsc{np} \backslash \textsc{s}}{}
     }
     \]
     \caption{Subject-relative intransitive verb derivation.}
     \end{subfigure}
     \begin{subfigure}[b]{1\textwidth}
         \centering
		\[
		\infer[/E]{\text{who}, \text{ate}, \text{an}, \text{apple} \vdash \textsc{np}\backslash \textsc{np}}{
			\infer[L]{\text{who} \vdash (\textsc{np} \backslash \textsc{np}) / (\textsc{np} \backslash \textsc{s})}{}
			&
			\infer[/E]{\text{ate}, \text{an}, \text{apple} \vdash \textsc{np}\backslash \textsc{s}}{
				\infer[L]{\text{ate} \vdash (\textsc{np}\backslash \textsc{s})/\textsc{np}}{}
				&
				\infer[/E]{\text{an}, \text{apple} \vdash \textsc{np}}{\dots}
			}
		}
		\]
         \caption{Subject-relative transitive verb derivation.}
	\end{subfigure}
	\begin{subfigure}[b]{1\textwidth}
         \centering
		\[
		\infer[/E]{\text{which}, \text{the}, \text{girl}, \text{ate} \vdash \textsc{np}\backslash \textsc{np}}{
			\infer[L]{\text{which} \vdash (\textsc{np} \backslash \textsc{np}) / (\textsc{s} / \textsc{np})}{}
			&
			\infer[/I]{\text{the}, \text{girl}, \text{ate} \vdash \textsc{s}/ \textsc{np}}{
				\infer[\backslash E]{\text{the}, \text{girl}, \text{ate}, \textsc{np} \vdash \textsc{s}}{
					\infer[/E]{\text{the}, \text{girl} \vdash \textsc{np}}{\dots}
					&
					\infer[/E]{\text{ate}, \textsc{np} \vdash \textsc{np} \backslash \textsc{s}}{
						\infer[L]{\text{ate} \vdash (\textsc{np} \backslash \textsc{s})/\textsc{np}}{}
						&
						\infer[Ax.]{\textsc{np} \vdash \textsc{np}}{}
					}
				}
			}
		}
		\]
        \caption{Object-relative transitive verb derivation.}
		\label{fig:lambek_en:obj}
     \end{subfigure}
     \caption[English Lambek Derivations]{Simple english sentences and their derivations using the Lambek Calculus. $L$ is used in place of Ax. for categories identified with lexical items.}
     \label{fig:lambek_en}
\end{figure}

\paragraph{Going Stricter}
In our presentation of the Lambek calculus, we have treated assumptions $\Gamma$, $\Delta$ as sequences, i.e. ordered collections, of formulas.
This can, at times, offer the grammar too much creative liberty, resulting in logically correct but linguistically wrong derivations.
This limitation can be bypassed by enhancing the logic with a notion of structure and set of rules to manipulate it, as originally proposed by Lambek~\cite{lambek1961calculus}.
Under this new regime, called the non-associative Lambek Calculus (NL), the logical rules are only applicable under the condition of appropriately bracketed structures:
\begin{align*}
    \begin{minipage}{0.5\textwidth}
    \begin{align*}
        \infer{(\Gamma \circ \Delta) \vdash B}{
            \Gamma \vdash B / A
            &
            \Delta \vdash A
        }\tag{/E}\\
        \\
        \infer{\Gamma \vdash B/A}{
            (\Gamma \circ A) \vdash B
        }\tag{/I}
    \end{align*}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \begin{align*}
        \infer[\backslash E]{(\Delta \circ \Gamma) \vdash B}{
            \Delta \vdash A
            &
            \Gamma \vdash A\backslash B
        }\tag{$\backslash E$}\\
        \\
        \infer[\backslash I]{\Gamma \vdash A\backslash B}{
            (A \circ \Gamma) \vdash B
        }\tag{$\backslash I$}
    \end{align*}
    \end{minipage}
\end{align*}

If the Lambek Calculus is a language of \textit{strings}, its non-associative version is the language of \textit{binary branching trees}; it  respects (and requires) the constituent structure of a phrase in providing its derivation.

\paragraph{Exerting Control}
Associativity and commutativity (in the form of permutation) may be added to the logic in the form of structural rules that enable them, thus obtaining L and LP (for Lambek Calculus with permutations)~\cite{van1988semantics}:
\begin{align*}
\infer{\Gamma[(\Delta_1 \circ \Delta_2) \circ \Delta_3] \vdash C}{\Gamma[\Delta_1 \circ (\Delta_2 \circ \Delta_3)]\vdash C} \tag{Associativity}\\
\infer{\Gamma[\Delta_1 \circ \Delta_2] \vdash C}{\Gamma[\Delta_2 \circ \Delta_1] \vdash C} \tag{Commutativity}
\end{align*}

Universally allowing associativity yields a grammar that loses track of constituent structurs, while universal commutativity corresponds to a grammar that completely ignores word order. 
Disallowing them altogether, on the on other hand, may be too harsh of a measure.
Notice, for instance, that the example on object-relativisation is no longer derivable without associativity, as seen in Figure~\ref{fig:nl_en:nd}.
To benefit from associativity and/or commutativity while restraining its applicability to only cases when it really is needed for a derivation, the logic may be expanded with unary modal operators that either allow or block structural rules~\cite{kurtonina1997structural}.
Structural control modalities improve the logic's proof-theoretic properties -- maximally, the only choices made available during proof search correspond to actual derivational ambiguity (i.e. non spurious).
That is, if more than one proofs may be devised for a single judgement, they correspond to its different readings.

To briefly illustrate the point, we can consider a pair of unary operators $\diamondsuit$, $\Box$ forming a residual pair such that $\diamondsuit \Box A \vdash A \vdash \Box \diamondsuit A$, with transitivity only applicable on elements marked with $\diamondsuit$:
\[
%\infer[A^{\diamondsuit}]{(A\circ B) \circ \diamondsuit C}{A \circ (B \circ \diamondsuit C)}
\] 

Then adding the type $(\textsc{np} \backslash \textsc{np}) / (\textsc{s}/\diamondsuit \Box \textsc{np})$ in our lexicon for the word ``which'' re-enables the derivability of the running example in Figure~\ref{fig:nl_en:d}.

\begin{figure}[t]
	\begin{subfigure}[b]{1\textwidth}
	\centering
	\[
	\infer[/E]{(\text{which} \circ ((\text{the} \circ \text{girl}) \circ \text{ate}))) \vdash \textsc{np}\backslash \textsc{np}}{
		\infer[L]{\text{which} \vdash (\textsc{np} \backslash \textsc{np}) / (\textsc{s} / \textsc{np})}{}
		&
		\infer[/I]{((\text{the} \circ \text{girl}) \circ \text{ate}) \vdash \textsc{s}/ \textsc{np}}{
			\infer[]{(((\text{the} \circ \text{girl}) \circ \text{ate}) \circ \textsc{np}) \vdash \textsc{s}}{
			\lightning
			}
		}
	}
	\]
	\caption[Non-derivable object-relative clause]{Non-derivable object-relative clause in NL.}		
	\label{fig:nl_en:nd}
	\end{subfigure}
	\begin{subfigure}[b]{1\textwidth}
		\centering
	\[
	\infer[/E]{(\text{which} \circ ((\text{the} \circ \text{girl}) \circ \text{ate}))) \vdash \textsc{np}\backslash \textsc{np}}{
		\infer[L]{\text{which} \vdash (\textsc{np} \backslash \textsc{np}) / (\textsc{s} / \diamondsuit \Box \textsc{np})}{}
		&
		\infer[/I]{((\text{the} \circ \text{girl}) \circ \text{ate}) \vdash \textsc{s}/ \diamondsuit \Box \textsc{np}}{
			\infer[A^{\diamondsuit}]{(((\text{the} \circ \text{girl}) \circ \text{ate}) \circ \diamondsuit \Box \textsc{np}) \vdash \textsc{s}}{
			\infer[/E]{((\text{the} \circ \text{girl}) \circ (\text{ate} \circ \diamondsuit \Box \textsc{np}))\vdash \textsc{s}}{
				\infer[\backslash E]{\text{the}, \text{girl}}{\dots}
				&
				\infer[\backslash E]{(\text{ate} \circ \diamondsuit \Box \textsc{np}) \vdash \textsc{np} \backslash \textsc{s}}{
					\infer[L]{\text{ate} \vdash (\textsc{np}\backslash \textsc{s})/\textsc{np}}{}
					&
					\infer[]{\diamondsuit \Box \textsc{np} \vdash \textsc{np}}{}
				}
			}
			}
		}
	}
	\]
	\caption{..now derivable using a residual pair of control operators.}
	\label{fig:nl_en:d}
	\end{subfigure}
	\caption[Structural Control Example]{Example of a structural control modality as a licensing feature for limited associativity.}
\end{figure}


\paragraph{Lexical Ambiguity}
These proof-theoretic advantages do not come for free, however.
Even though a stricter type system mitigates the difficulty of proof search, the burden is not removed but rather shifted onto the lexicon.
As words are assigned more potential types, the lexicon becomes increasingly ambiguous.
This ambiguity is further exacerbated for languages exhibiting higher degrees of word order freedom, with Dutch being a prime example.

To evaluate what the practical impact of word order freedom is, we will start by making a naive translation of our prior lexicon into Dutch.

 \[
 \begin{array}{cc}
 \text{Word} & \text{Category} \\
 \hline
 \text{meisje}\ (\textit{girl}), \ \text{appel} \ (\textit{apple}) & \textsc{n} \\
 \text{kinderen} \ (\textit{children}) & \textsc{np} \\
 \text{het} \ (\textit{the}) , \ \text{een} \ (\textit{an}) & \textsc{np}/ \textsc{n} \\
 \text{spelen} \ (\textit{play}), \ \text{spelt} \ (\textit{plays}) & \textsc{np}\backslash \textsc{s} \\
 \text{at} \ (\textit{ate}) & (\textsc{np} \backslash \textsc{s}) / \textsc{np} \\ 
 \text{dat} \ (\textit{who$^{SG}$}), \ \text{die} \ (\textit{who$^{PL}$}) & (\textsc{np} \backslash \textsc{np}) / (\diamondsuit \Box \textsc{np} \backslash \textsc{s}) \\
 \text{die} \ (\textit{which}) & (\textsc{np} \backslash \textsc{np})/(\textsc{s} / \diamondsuit \Box \textsc{np})
 \end{array}
 \]

Things look promising at first; the sentences ``kinderen spelen'' (\textit{children play}) and ``het meisje at een appel''  (\textit{the girl ate an apple}) are derivable, and so is the phrase ``die spelen'' (\textit{who play}), with proofs identical to the first three of Figure~\ref{fig:lambek_en}.
However, we reach an impasse when trying to create a proof for ``dit een appel at'' (\textit{that ate an apple}) with our current lexicon, even with our transitivity-licensing relativiser, as shown in Figure~\ref{fig:nl_dutch}.
\begin{figure}
	\centering
		\[
		\infer[/E]{(\text{dat} \circ ((\text{een} \circ \text{appel}) \circ \text{at})) \vdash \textsc{np}\backslash \textsc{np}}{
			\infer[L]{\text{dat} \vdash (\textsc{np} \backslash \textsc{np}) / (\diamondsuit \Box \textsc{np} \backslash \textsc{s})}{}
			&
			\infer[\backslash I]{((\text{een} \circ \text{appel}) \circ \text{at}) \vdash \diamondsuit \Box \textsc{np}\backslash \textsc{s}}{
				\infer[A^\diamondsuit]{(\diamondsuit \Box \textsc{np} \circ ((\text{een} \circ \text{appel}) \circ \text{at}))}{
					\infer[]{((\diamondsuit \Box \textsc{np} \circ (\text{een} \circ \text{appel})) \circ \text{at})}{\lightning}
				}
			}
		}
		\]
\caption[Failing NL Derivation]{Non-derivable subject-relativisation example in Dutch.}
\label{fig:nl_dutch}
\end{figure}
The issue arises because relativisation in Dutch is verb-final, requiring either the additional type $\textsc{np}\backslash(\textsc{np}\backslash\textsc{s})$ for transitive verbs, or modal decorations for conditional commutativity and associativity that would allow the hypothetical noun-phrase to move unhindered through the proof until it finds its correct position.

Similar complications are common, drastically increasing the complexity required of the type system if we want proof search to remain deterministic.
This complexity can become unwieldy when dealing with large-scale corpora, owing mostly to the potentially immense size (and therefore ambiguity) of the lexicon, but also the considerable difficulty of populating such a lexicon in the first place.
Consequently, there is a balance between to be sought between the formal well-behavedness of the grammar and its practical applicability.



\section{A TLG for Semantic Compositionality}
\subsection{Intuitionistic Linear Logic}
The above considerations, combined with our goal of constructing a wide-coverage grammar for Dutch, draws our attention towards LP, otherwise known as the Lambek-van Benthem Calculus~\cite{van1988semantics}.
LP coincides with the implication-only fragment of Intuitionistic Linear Logic (ILL)~\cite{girard1987linear} the notation of which is adopted for the purposes of this presentation.
ILL is in many ways reminiscent of the Lambek Calculus as shown earlier; the two logical connectives of $/$ and $\backslash$ are collapsed into a single, direction-agnostic implication $\rightarrow$ (an alternative notation is $\multimap$).
Types are then inductively defined as:
\[
\textsc{t} := \textsc{a} \ | \ \textsc{t}_1 \to \textsc{t}_2 
\]
where again $\textsc{a}$ is an atomic type and $\textsc{t}_1$, $\textsc{t}_2$ are types.

The implication rules and identity axiom of ILL are presented below:

\begin{center}
\begin{minipage}{0.5\textwidth}
\[
\infer{A \vdash A}{}\tag{Ax.}
\]
\end{minipage}\\
\end{center}
\begin{align*}
    \begin{minipage}{0.5\textwidth}
	\[
        \infer{\Gamma, \Delta \vdash B}{
            \Gamma \vdash A \rightarrow B
            &
            \Delta \vdash A
        }\tag{$\rightarrow$ E}
    \]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \[
        \infer{\Gamma \vdash A \rightarrow B}{
            \Gamma, A \vdash B
        }\tag{$\rightarrow I$}\\
    \]
    \end{minipage}
\end{align*}

Note that rather than the sequences of L or binary branching structures of NL, the assumptions of a judgement in ILL are now \textit{multisets}; $\Gamma$, $\Delta$ is then read as the multiset union of $\Gamma$ with $\Delta$ and is equivalent to $\Delta$, $\Gamma$.
In practical terms, this means that both associativity and commutativity are admitted as holding universally; neither structure nor word order are taken into account when deriving a sentence\footnote{For non-native Dutch speakers, this may at times feel as less of a concession and more of a postulate.}.

\paragraph{The Curry-Howard Correspondence}
The Curry-Howard Correspondence states that logical propositions are in a one-to-one relation with the types of a functional program.
ILL, in particular, is directly equivalent to the simply-typed linear $\lambda$-calculus~\cite{benton1993term, abramsky1993computational}.
A proof then encodes a $\lambda$-term which fully specifies the execution of a functional program, and vice-versa.
Logical connectives are identified with type constructors; specifically, implicational formulas are the type signatures of function spaces. 
Assumptions are free variables, and the rules of introduction and elimination find their computational analogues in function abstraction and function application, respectively, whereas the identity axiom corresponds to variable instantiation.

The ramifications of this insight are far-reaching and their analysis falls beyond the scope of this thesis; we rather want to focus on one particular aspect of the correspondence, namely its significance for semantic compositionality.

Let's begin by inspecting the logical rules decorated with their corresponding $\lambda$-terms, and giving them an intuitive reading.

\begin{center}
\begin{minipage}{0.5\textwidth}
\[
\infer{x: A \vdash x: A}{}\tag{Ax.}
\]
\end{minipage}\\
\end{center}
\begin{align*}
    \begin{minipage}{0.5\textwidth}
	\[
        \infer{\Gamma, \Delta \vdash s(t): B}{
            \Gamma \vdash s: A \rightarrow B
            &
            \Delta \vdash t: A
        }\tag{$\rightarrow$ E}
    \]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \[
        \infer{\Gamma \vdash \lambda x. u : A \rightarrow B}{
            \Gamma, x: A \vdash u: B
        }\tag{$\rightarrow I$}\\
    \]
    \end{minipage}
\end{align*}

The identity axiom simply states that we may instantiate a free variable $x$ of type $A$.
The elimination rule $\rightarrow E$ states that if we have a program that from a set of typed variables $\Gamma$ can produce a function $s$ of type $A \to B$, and a program that from another set $\Delta$ can produce a variable $t$ of type $A$, then $s$ may be applied to $t$ yielding a variable of type $B$.
Dually, the introduction rule states that from a program that produces a variable $u$ of type $B$ out of a set of variables $\Gamma$ together with a variable $x$ of type $A$, we can construct a program for a function $A\to B$ by abstracting $x$ away.

Recalling that our variables are instantiated by a type lexicon, we can easily shift from syntax to semantics via an isomorphic mapping.
Concretely, for each lexical type assignment we need also provide a corresponding semantic assignment.
Then, the process of meaning assembly for a phrase is identical with the execution of the functional program dictated by its syntactic derivation; in other words, we may use the semantic values of our lexicon, applying function-words to their arguments in a hierarchical manner, guided by the $\lambda$-term that encodes the proof structure.

As semantic compositionality is one of the planned applications of our grammar, we will stress this point by providing an abstract example.
First, let's return to the prior examples, now using an ILL-adapted lexicon\footnote{Throughout the remainder of this thesis, we will use a right-implicit parentheses notation for ILL types; that is, $A\to B \to C$ is read as $A\to (B \to C)$ and is distinct from $(A\to B) \to C$}:

 \[
 \begin{array}{cc}
 \text{Word} & \text{Category} \\
 \hline
 \text{meisje}, \ \text{appel} & \textsc{n} \\
 \text{het}, \ \text{een}  & \textsc{n} \to \textsc{np}\\
 \text{at}  & \textsc{np} \to \textsc{np} \to \textsc{s}\\ 
 \text{dat}, \ \text{die} & (\textsc{np}\to \textsc{s})\to \textsc{np} \to \textsc{np} \\
 \end{array}
 \]

Figure~\ref{fig:ill_dutch} presents derivations for a transitive verb phrase in primary and embedded clauses.
The corresponding $\lambda$-terms are obtained by following the proof constructions top-down.
Note that the embedded clause example that gave us trouble earlier is now trivial to derive, while maintaining the subject-relative reading.

\begin{figure}
	\begin{subfigure}[b]{1\textwidth}
	\centering
	\small
		\[
		\infer[\rightarrow E]{\text{het}, \text{meisje}, \text{at}, \text{een}, \text{appel} \vdash \textsc{s}}{
			\infer[\rightarrow E]{\text{at}, \text{een}, \text{appel} \vdash \textsc{np} \to \textsc{s}}{
				\infer[L]{\text{at} \vdash \textsc{np} \to \textsc{np} \to \textsc{s}}{}
				&
				\infer[\rightarrow E]{\text{een}, \text{appel} \vdash \textsc{np}}{
					\infer[L]{\text{een} \vdash \textsc{n} \to \textsc{np}}{}
					&
					\infer[L]{\text{appel} \vdash \textsc{n}}{}
				}
			}
			&		
			\infer[\rightarrow E]{\text{het}, \text{meisje} \vdash \textsc{np}}{
				\infer[L]{\text{het} \vdash \textsc{n} \to \textsc{np}}{}
				&
				\infer[L]{\text{meisje} \vdash \textsc{n}}{}
			}
		}
		\]
		\caption{Simple transitive verb derivation, with $\lambda$-term (at(een appel))(het meisje)}
		\end{subfigure}
		\begin{subfigure}[b]{1\textwidth}
		\centering
		\scriptsize
		\[
		\infer[\rightarrow E]{\text{dat}, \text{een}, \text{appel}, \text{at} \vdash \textsc{np} \to \textsc{np}}{
			\infer[L]{\text{dat} \vdash (\textsc{np} \to \textsc{s}) \to \textsc{np} \to \textsc{np}}{}
			&
			\infer[\rightarrow E]{\text{een}, \text{appel}, \text{at} \vdash \textsc{np} \to \textsc{s}}{
				\infer[L]{\text{at} \vdash \textsc{np} \to \textsc{np} \to \textsc{s}}{}
				&
				\infer[\rightarrow E]{\text{een}, \text{appel} \vdash \textsc{np}}{
					\infer[L]{\text{een} \vdash \textsc{n} \to \textsc{np}}{}
					&
					\infer[L]{\text{appel} \vdash \textsc{n}}{}
				}
			}
		}
		\]
		\caption{Subject-relative transitive verb derivation, with $\lambda$-term dat(at (een appel))}
		\label{subfig:ill_dutch:sub}
		\end{subfigure}
		\begin{subfigure}[b]{1\textwidth}
		\centering
		\scriptsize
		\[
		\infer[\rightarrow E]{\text{die}, \text{het}, \text{meisje}, \text{at} \vdash \textsc{np} \to \textsc{np}}{
			\infer[L]{\text{die} \vdash (\textsc{np} \to \textsc{s})\to \textsc{np} \to \textsc{np}}{}
			&
			\infer[\rightarrow I]{\text{het}, \text{meisje}, \text{at} \vdash \textsc{np} \to \textsc{s}}{
				\infer[\rightarrow E]{\text{het}, \text{meisje}, \text{at}, \textsc{np} \vdash \textsc{s}}{
					\infer[\rightarrow E]{\text{at}, \textsc{np}\vdash \textsc{np} \to \textsc{s}}{
						\infer[L]{\text{at} \vdash \textsc{np} \to \textsc{np} \to \textsc{s}}{}
						&
						\infer[Ax.]{\textsc{np} \to \textsc{np}}{}
					}
					&
					\infer[\rightarrow E]{\text{het}, \text{meisje} \vdash \textsc{np}}{
						\infer[L]{\text{het} \vdash \textsc{n} \to \textsc{np}}{}
						&
						\infer[L]{\text{meisje} \vdash \textsc{n}}{}					
					}
				}
			}
		}
		\]
		\caption{Object-relative transitive verb derivation, with $\lambda$-term dit($\lambda x$.((at $x$)(het meisje)))}
		\label{subfig:ill_dutch:obj}
		\end{subfigure}
\caption[Example ILL Derivations]{Example derivations in ILL}
\label{fig:ill_dutch}
\end{figure}

Now, given a mapping $\lceil .\rceil$ from words to semantic objects, we may obtain a compositionally driven semantic interpretation over larger linguistic units by recursively applying the mapping on the proofs' $\lambda$-terms, e.g.:
\[
\lceil \text{dit het meisje at} \rceil = \lceil \text{dit} \rceil (\lambda x. ((\lceil \text{at} \rceil \ x)(\lceil \text{het} \rceil \lceil \text{meisje} \rceil))
\]

The exact semantic spaces operated on are still open to our creative libery; specifying those escapes the context of this thesis, but the grammar's ability to accommodate a multitude of such spaces is still an important point to consider.
A final remark to make is that a compositional semantics is not particular to this instantiation of our type-logic. 
Any Lambek-style calculus can be converted into an ILL program for meaning assembly via a homomorphic translation; the difference here is that this translation is invertible as both syntax and semantics share the same structure.

\subsection{Dependency Refinement}
Earlier, we saw how ILL simplifies the derivation process for cases that would otherwise require involved structural reasoning.
The observant reader will, however, have noticed that this laxity can result in erroneous analyses.
Returning, for instance, to the relativisation examples of Figure~\ref{fig:ill_dutch}, there is no restriction enforcing us to derive the particular proofs presented.
The proof structure of~\ref{subfig:ill_dutch:sub} could be used in~\ref{subfig:ill_dutch:obj} (and vice-versa), resulting in linguistically inaccurate readings.
Even though associativity and commutativity mitigate the problem of resolving long-distance or crossing dependencies, they vastly increase the proof-search space and permit derivations that are completely off-point.

This is a compromise consciously made; even though the types will not suffice for deductive parsing, they may be used as an auxiliary information source on top of the words themselves.
The assumption made here is that the combination of lexical-level preferences and type-level information will prove adequate in the selection of the most plausible reading and its proof; for example, when concerned with the sentence ``het meisje at een appel'' (\textit{the girl ate an apple}), we know with a degree of certainty that apples are much more plausible as objects of eating compared to girls.

The crucial insight is that even though the position of phrasal dependants (and thus functor arguments) may be variable, their syntactic role remains constant.
These roles are left implicit for non-permuting calculi (for example the inner-most argument of the $\textsc{np}\backslash(\textsc{s}/\textsc{np})$ refers to the object of the transitive verb), but can be explicitly defined in the current setting.
To imlement this refinement, we subclass the implication arrow into several named variants, each (roughly) corresponding to a particular dependency label.
Functor types now also specify the syntactic slot occupied by their arguments, denoted by the name decoration of their corresponding implication.
A transitive verb then gets typed as $\textsc{np}\myrightarrow{su}\textsc{np}\myrightarrow{obj}\textsc{s}$; a curried function that consumes first a noun-phrase in subject position and then a noun-phrase in object position before producing a sentence.

Such an enrichment of the type system has multifold benefits; functor types gain a more intuitive reading, which also greatly increases their informational content.
They no longer encode just the local phrase structure, but also the dependencies enacted by the structure; functors that share the same arguments but have them occupy different syntactic slots are now distinguishable from one another.
Further, lexical preferences can now be canonically shared across types rather than being tied to argument positions over individual types.
Last but not least, novel usecases for dependency decorations are likely to arise in the context of semantic interpretations.

Of course, this addition is expected to significantly increase the lexicon size.
However, differently decorated types arising from a single ILL type will really be functionally distinct (both syntactically and semantically), as opposed to simply an artifact of permutation (as would be the case in a directional system).
Consider, for instance, the case of the relativiser ``die'', which would get the two different types\footnote{The dependency label body refers to relative clause body.} $(\textsc{np}\myrightarrow{su}\textsc{s})\myrightarrow{body}\textsc{np}\myrightarrow{mod}\textsc{np}$ and $(\textsc{np}\myrightarrow{obj}\textsc{s})\myrightarrow{body}\textsc{np}\myrightarrow{mod}\textsc{np}$, the first for subject- and the second for object-relativisation.
The size increase is anyway modest compared to what a directional grammar would yield; therefore, the decorations are cost-efficient, in the sense that they offer the most advantages for the least added complexity and lexical ambiguity.

It is noteworthy to point out that not all implication instances need be decorated; higher-order types might involve hypothetical reasoning over objects which do not project dependency information.

\paragraph{A Formal Treatment}
Throughout the rest of the thesis, we will be using the decsribed dependency decorations in a informal manner, as a meta-logical notation that is simply used to convey useful auxiliary information.
However, such decorations can be properly included in the logic in the form of modal operators.
First, consider that the arrow decoration is a characterization of the means of argument consumption; as such, it can be shifted on to the argument itself.
With this in mind, we can insert a unary logical connective $\diamondsuit^d$ for each dependency label $d$, together with its corresponding structural counterpart $\langle \rangle ^d$.

The logic then requires two extra rules for $\diamondsuit^d$ Introduction and Elimination:

\begin{align*}
    \begin{minipage}{0.5\textwidth}
	\[
        \infer{\langle \Gamma \rangle^d \vdash \diamondsuit^d A}{\Gamma \vdash A}\tag{$\diamondsuit^d I$}
    \]
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \[
        \infer{\Gamma [\Delta] \vdash B}{
        \Delta \vdash \diamondsuit^d A
        &
        \Gamma[\langle A \rangle^d] \vdash B
        }\tag{$\diamondsuit^d E$}
    \]
    \end{minipage}
\end{align*}

The first states that a proof of $A$ from assumptions $\Gamma$ can be converted into a proof a $d$-decorated $A$ from a $d$-bracketed $\Gamma$.
The second says that given a proof for $\diamondsuit^d A$ from structure $\Delta$ and a proof of $B$ from a structure $\Delta$ that contains a $d$-bracketed $A$ as a sub-structure, the latter may be replaced by a $\Gamma$ and still yield a $B$.
Figure~\ref{fig:ill_dutch_modal} presents another relativisation example, showcasing the effect of dependency decorations in the proof.
Note that the modalities do not alter proof-search when performed in a forward manner, but require the dependency bracketing when done in a backward manner.
A two-step backward-forward strategy may be used to first construct a standard ILL proof, then insert the modal decorations.

\begin{figure}
	\begin{sideways}
	\begin{subfigure}[lt]{1\textheight}
	\centering
	\scriptsize
	\[
	\infer{\langle \text{eieren} \rangle^\text{mod}, \text{die}, \langle \langle \text{kippen} \rangle^\text{obj}, \text{maken} \rangle^\text{body} \vdash \textsc{np}}{
		\infer[\rightarrow E]{\text{die}, \langle \langle \text{kippen} \rangle^\text{obj}, \text{maken}\rangle^\text{body} \vdash \diamondsuit^\text{mod}\textsc{np} \rightarrow \textsc{np}}{
			\infer[L]{\text{die} \vdash \diamondsuit^\text{body}(\diamondsuit^\text{su}\textsc{np} \to \textsc{s})\to(\diamondsuit^\text{mod} \textsc{np} \to \textsc{np})}{}
			&
			\hspace{-20pt}
			\infer[\diamondsuit^\text{body}I]{\langle \langle \text{kippen} \rangle^\text{obj}, \text{maken} \rangle^\text{body} \vdash \diamondsuit^\text{body}(\diamondsuit^\text{su}\textsc{np} \to \textsc{s})}{
				\infer[\rightarrow I]{\langle \text{kippen} \rangle^\text{obj}, \text{maken}  \vdash \diamondsuit^\text{su} \textsc{np}\to \textsc{s}}{
					\infer[\diamondsuit^\text{su} E]{\langle \text{kippen} \rangle^\text{obj}, \text{maken}, \diamondsuit^\text{su} \textsc{np} \vdash \textsc{s}}{
						\infer[Ax.]{\diamondsuit^\text{su} \textsc{np} \vdash \diamondsuit^\text{su} \textsc{np}}{}
						&
						\infer[\rightarrow E]{\langle \text{kippen} \rangle^\text{obj}, \text{maken}, \langle \textsc{np} \rangle^\text{su} \vdash \textsc{s}}{
							\infer[\rightarrow E]{\text{maken}, \diamondsuit^\text{su} \vdash \diamondsuit^\text{obj} \textsc{np} \to \textsc{s}}{
								\infer[L]{\text{maken} \vdash \diamondsuit^\text{su}\textsc{np}\to\diamondsuit^\text{obj} \textsc{np} \to \textsc{s}}{}
								&
								\infer[Ax.]{\diamondsuit^\text{su}\textsc{np} \vdash \diamondsuit^\text{su}\textsc{np}}{}
							}
							&
							\infer[\diamondsuit^\text{obj} I]{\langle \text{kippen} \rangle^\text{obj}\vdash \diamondsuit^\text{obj}\textsc{np}}{
							\infer[L]{\text{kippen} \vdash \textsc{np}}{}
							}
						}
					}
				}
			}
		}
		&
		\hspace{-90pt}
		\infer[\diamond^\text{mod}I]{\langle \text{eieren} \rangle^\text{mod} \vdash \diamondsuit^\text{mod} \textsc{np}}{
			\infer[L]{\text{eieren} \vdash \textsc{np}}{}
		}
	}
	\]
	\caption{Subject-relative derivation.}
	\label{subfig:modal_su}
	\end{subfigure}
	\end{sideways}
		\begin{sideways}
	\begin{subfigure}[lb]{1\textheight}
	\centering
	\scriptsize
	\[
	\infer{\langle \text{eieren} \rangle^\text{mod}, \text{die}, \langle \langle \text{kippen} \rangle^\text{su}, \text{maken} \rangle^\text{body} \vdash \textsc{np}}{
		\infer[\rightarrow E]{\text{die}, \langle \langle \text{kippen} \rangle^\text{su}, \text{maken}\rangle^\text{body} \vdash \diamondsuit^\text{mod}\textsc{np} \rightarrow \textsc{np}}{
			\infer[L]{\text{die} \vdash \diamondsuit^\text{body}(\diamondsuit^\text{obj}\textsc{np} \to \textsc{s})\to(\diamondsuit^\text{mod} \textsc{np} \to \textsc{np})}{}
			&
			\infer[\diamondsuit^\text{body}I]{\langle \langle \text{kippen} \rangle^\text{su}, \text{maken} \rangle^\text{body} \vdash \diamondsuit^\text{body}(\diamondsuit^\text{obj}\textsc{np} \to \textsc{s})}{
				\infer[\rightarrow E]{\langle \text{kippen} \rangle^\text{su}, \text{maken} \vdash \diamondsuit^\text{obj}\textsc{np} \to \textsc{s}}{
					\infer[L]{\text{maken} \vdash \diamondsuit^\text{su} \textsc{np} \to \diamondsuit^\text{obj} \textsc{np} \to \textsc{s}}{}
					&
					\infer[\diamondsuit^\text{su}I]{\langle \text{kippen} \rangle^\text{su} \vdash \diamondsuit^\text{su} \textsc{np}}{
						\infer[L]{\text{kippen} \vdash \textsc{np}}{}
					}
				}
			}
		}
		&
		\infer[\diamond^\text{mod}I]{\langle \text{eieren} \rangle^\text{mod} \vdash \diamondsuit^\text{mod} \textsc{np}}{
			\infer[L]{\text{eieren} \vdash \textsc{np}}{}
		}
	}
	\]
	\caption{Object-relative derivation.}
	\label{subfig:modal_obj}
	\end{subfigure}
	\end{sideways}
\caption[Example ILL Derivations with Dependency Modalities]{Example derivations in ILL, using dependency modalities, for the sentence ``eieren die kippen maken''. Subfigure~\ref{subfig:modal_su} presents the subject-relative reading (\textit{eggs that make chickens}), while subfigure~\ref{subfig:modal_obj} presents the object-relative reading (\textit{eggs that chickens make}).}
\label{fig:ill_dutch_modal}
\end{figure}

\section{Summary}
This chapter examined Type-Logical Grammars and some of the major steps throughout their evolution.
We saw how parsing may be understood as a formal deduction process, and how stricter logics can regulate this process so as to keep it linguistically grammatical.
Using a few examples, we recognized that stricter logics come at the cost of increased lexical ambiguity, further emphasized for a language with free word order like Dutch. 
With this in mind, we considered a laxer grammar based on Intuinistic Linear Logic, which is this Chapter's novel contribution.

An ILL-based grammar boasts simplicity, ease of dealing with discontinuous or long-range dependencies and a clear and direct correspondence with the simply typed linear $\lambda$-calculus, making it an ideal driving force for semantic compositionality.
On the other hand, the axiomatic treatment of associativity and commutativity permits more proofs than desired and increases parsing complexity.
To address this issue, we enriched the grammar by subclassing the linear implication to a set of named versions, each one suggesting a unique ``means of consumption'' as specified by a corresponding syntactic dependency. 
This addition can facilitate a preferential lexical bias, which can be implemented by statistical or heuristic means, aiding in a reduction of the search space ambiguity.